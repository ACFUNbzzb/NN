import os
import numpy as np
import pandas as pd
import shap
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split
from matminer.featurizers.composition import ElementProperty
from matminer.featurizers.conversions import StrToComposition
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
from scipy.stats import spearmanr, pearsonr


# ========= æ¨¡å—1ï¼šå‡†å¤‡åŒ–å­¦å¼æ•°æ®å¹¶æå–ç‰¹å¾ =========
def prepare_multi_system_inputs():
    """å‡†å¤‡åŒ–å­¦å¼æ•°æ®å¹¶æå–ç‰¹å¾"""
    system_bases = [
        'Li26Ni27FeO54', 'Li25Ni27Fe2O54', 'Li24Ni27Fe3O54', 'Li23Ni27Fe4O54', 'Li22Ni27Fe5O53',
        'Li26Ni27O54', 'Li27Ni27O54', 'Li27Ni27O53', 'Li27Ni27O52', 'Li25Ni27CoMnO54',
        'Li26Ni27CoO54', 'Li25Ni27Co2O54', 'Li24Ni27Co3O54', 'Li23Ni27Co4O54', 'Li22Ni27Co5O53',
        'Li26Ni27MnO54', 'Li25Ni27Mn2O54', 'Li24Ni27Mn3O54', 'Li23Ni27Mn4O54', 'Li22Ni27Mn5O53',
        'Li26Ni27AlO54', 'Li25Ni27Al2O54', 'Li24Ni27Al3O54', 'Li23Ni27Al4O54', 'Li22Ni27Al5O53',
        # å‡è®¾ä½ æœ‰50ä¸ªçœŸå®æ ·æœ¬ï¼Œç»§ç»­è¡¥å……åˆ—è¡¨
    ]
    featurizer = ElementProperty.from_preset("magpie", impute_nan=True)
    X_all, y_all = [], []

    for formula in system_bases:
        try:
            # å°†åŒ–å­¦å¼è½¬æ¢ä¸ºåŒ–å­¦æˆåˆ†
            df = pd.DataFrame({'formula': [formula]})
            df = StrToComposition().featurize_dataframe(df, col_id='formula', ignore_errors=True)
            # æå–åŒ–å­¦æˆåˆ†ç‰¹å¾
            df = featurizer.featurize_dataframe(df, col_id='composition', ignore_errors=True)
            # è·å–ç‰¹å¾å€¼
            features = df.drop(columns=['formula', 'composition']).values[0]
            X_all.append(features)
            # æ¨¡æ‹Ÿæ ‡ç­¾ï¼šåŸºäºå½“å‰æ ·æœ¬çš„ç´¢å¼•ç”Ÿæˆä¸€ä¸ªæ ‡ç­¾
            y_all.append(0.4 + 0.01 * len(y_all))
        except Exception as e:
            # å‘ç”Ÿé”™è¯¯æ—¶æ‰“å°å‡ºé”™åŒ–å­¦å¼
            print(f"âŒ å¤„ç†åŒ–å­¦å¼ {formula} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            continue

    # å°†æ‰€æœ‰ç‰¹å¾æ•°æ®å’Œæ ‡ç­¾åˆå¹¶ä¸º numpy æ•°ç»„
    X_merged = np.array(X_all)
    y_merged = np.array(y_all)
    print("\U0001F4CA ç‰¹å¾åˆ†å¸ƒç»Ÿè®¡ä¿¡æ¯ï¼š")
    print(pd.DataFrame(X_merged).describe())

    return X_merged, y_merged


# ========= æ¨¡å—2ï¼šæ ‡å‡†åŒ–æ•°æ® =========
def standardize_and_align(X_raw, y_array):
    """æ ‡å‡†åŒ–ç‰¹å¾"""
    # ä½¿ç”¨StandardScalerå¯¹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–ï¼Œä½¿å…¶å‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1
    scaler = StandardScaler().fit(X_raw)
    X_scaled = scaler.transform(X_raw)
    return X_scaled, y_array, X_raw.shape[1], scaler


# ========= æ¨¡å—3ï¼šè‡ªå®šä¹‰æ•°æ®é›†ç»“æ„ =========
class MultiSystemDataset(Dataset):
    """PyTorchæ•°æ®é›†å°è£…"""
    def __init__(self, X, y):
        self.X = X
        self.y = y

    def __len__(self):
        return len(self.y)

    def __getitem__(self, idx):
        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.float32)


# ========= æ¨¡å—4ï¼šå®šä¹‰ç¥ç»ç½‘ç»œç»“æ„ =========
class BatteryModel(nn.Module):
    """ç®€å•MLPå›å½’æ¨¡å‹"""
    def __init__(self, input_dim, hidden_units=64):
        super(BatteryModel, self).__init__()
        # å®šä¹‰ä¸€ä¸ªç®€å•çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼ˆMLPï¼‰
        self.fc = nn.Sequential(
            nn.Linear(input_dim, hidden_units),  # è¾“å…¥å±‚åˆ°éšè—å±‚
            nn.ReLU(),  # æ¿€æ´»å‡½æ•°
            nn.Linear(hidden_units, hidden_units),  # ç¬¬äºŒä¸ªéšè—å±‚
            nn.ReLU(),  # æ¿€æ´»å‡½æ•°
            nn.Linear(hidden_units, 1)  # è¾“å‡ºå±‚
        )

    def forward(self, x):
        return self.fc(x)


# ========= æ¨¡å—5ï¼šæ•°æ®æ‰©å……å‡½æ•° =========
def augment_with_noise(X, y, target_size=100, noise_level_X=0.01, noise_level_y=0.005):
    """æ‰©å……æ•°æ®åˆ°æŒ‡å®šæ•°é‡ï¼ˆå¼ºåˆ¶è®¾ç½®ä¸º100ï¼‰ï¼Œå¹¶æ·»åŠ å°å¹…å™ªå£°"""
    X_aug, y_aug = [X.copy()], [y.copy()]
    rng = np.random.default_rng()

    # è®¡ç®—å½“å‰æ•°æ®é›†çš„å¤§å°
    current_size = X.shape[0]

    # å¦‚æœå½“å‰æ•°æ®é‡å°äºç›®æ ‡æ•°é‡ï¼ŒæŒç»­æ‰©å……æ•°æ®
    while len(X_aug) * current_size < target_size:
        noise_X = rng.normal(0, noise_level_X, X.shape)  # å¯¹ç‰¹å¾æ·»åŠ å™ªå£°
        noise_y = rng.normal(0, noise_level_y, y.shape)  # å¯¹æ ‡ç­¾æ·»åŠ å™ªå£°
        X_new = X + noise_X
        y_new = y + noise_y
        X_aug.append(X_new)
        y_aug.append(y_new)

    # æœ€ç»ˆæ•°æ®é›†å¤§å°è®¾ç½®ä¸ºç›®æ ‡å¤§å°
    X_final = np.vstack(X_aug)[:target_size]
    y_final = np.hstack(y_aug)[:target_size]
    return X_final, y_final


# ========= ä¸»ç¨‹åºå…¥å£ =========
if __name__ == '__main__':
    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    os.makedirs("training_output", exist_ok=True)
    os.makedirs("prediction_output", exist_ok=True)
    os.makedirs("saved_model", exist_ok=True)

    shap.initjs()  # å¯ç”¨SHAPå¯è§†åŒ–å·¥å…·

    # åŸå§‹æ•°æ®å‡†å¤‡
    X_raw, y_raw = prepare_multi_system_inputs()
    X_scaled, y, input_dim, scaler = standardize_and_align(X_raw, y_raw)

    # ç¬¬ä¸€æ­¥ï¼šåˆ’åˆ†15%çš„åŸå§‹æ•°æ®ä½œä¸ºæµ‹è¯•é›†
    X_trainval_raw, X_test_raw, y_trainval_raw, y_test_raw = train_test_split(X_scaled,
                                                                              y, test_size=0.15, random_state=42)

    # ç¬¬äºŒæ­¥ï¼šæ‰©å……è®­ç»ƒé›†åˆ°100ä¸ªæ ·æœ¬
    X_trainval_aug, y_trainval_aug = augment_with_noise(X_trainval_raw, y_trainval_raw, target_size=100)

    # è®°å½•æ¯æŠ˜çš„è¯„ä¼°æŒ‡æ ‡
    all_metrics = {
        'mse': [],
        'mae': [],
        'r2': [],
        'spearman': [],
        'pearson': []
    }

    # è¿›è¡Œ10æ¬¡äº¤å‰éªŒè¯
    for fold in range(1, 11):
        print(f"\nğŸš€ å¼€å§‹ç¬¬ {fold}/10 æ¬¡äº¤å‰éªŒè¯")

        # ä»æ‰©å……çš„è®­ç»ƒé›†ä¸­åˆ’åˆ†éªŒè¯é›†
        X_train, X_val, y_train, y_val = train_test_split(X_trainval_aug, y_trainval_aug, test_size=0.15, random_state=fold*10)

        # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
        train_dataset = MultiSystemDataset(X_train, y_train)
        val_dataset = MultiSystemDataset(X_val, y_val)
        test_dataset = MultiSystemDataset(X_test_raw, y_test_raw)  # æµ‹è¯•é›†ä¸€ç›´æ˜¯çœŸå®åŸå§‹æ•°æ®ï¼

        train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=8)
        test_loader = DataLoader(test_dataset, batch_size=8)

        # å®šä¹‰æ¨¡å‹ã€ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        model = BatteryModel(input_dim, hidden_units=64)
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        loss_fn = nn.MSELoss()

        best_val_loss = float('inf')  # åˆå§‹æœ€ä¼˜éªŒè¯æŸå¤±
        train_losses, val_losses = [], []  # å­˜å‚¨è®­ç»ƒå’ŒéªŒè¯æŸå¤±

        for epoch in range(100):
            model.train()
            total_loss = 0
            for X_batch, y_batch in train_loader:
                optimizer.zero_grad()
                pred = model(X_batch)
                loss = loss_fn(pred, y_batch.view(-1, 1))  # è®¡ç®—æŸå¤±
                loss.backward()  # åå‘ä¼ æ’­
                optimizer.step()  # ä¼˜åŒ–æ­¥éª¤
                total_loss += loss.item()

            train_losses.append(total_loss / len(train_loader))  # è®°å½•è®­ç»ƒæŸå¤±

            # éªŒè¯æŸå¤±
            model.eval()
            val_loss = 0
            preds, trues = [], []
            with torch.no_grad():
                for X_batch, y_batch in val_loader:
                    pred = model(X_batch)
                    val_loss += loss_fn(pred, y_batch.view(-1, 1)).item()
                    preds.append(pred)
                    trues.append(y_batch)

            avg_val_loss = val_loss / len(val_loader)
            val_losses.append(avg_val_loss)  # è®°å½•éªŒè¯æŸå¤±

            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss  # æ›´æ–°æœ€ä¼˜éªŒè¯æŸå¤±

        # ä¿å­˜æ¯æŠ˜çš„æ¨¡å‹
        torch.save(model.state_dict(), f'saved_model/model_fold_{fold}.pt')

        # æµ‹è¯•è¯„ä¼°
        model.eval()
        X_test_tensor = torch.tensor(X_test_raw, dtype=torch.float32)
        y_test_tensor = torch.tensor(y_test_raw, dtype=torch.float32).view(-1, 1)
        with torch.no_grad():
            y_pred = model(X_test_tensor).numpy()

        # ç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿
        plt.figure()
        plt.plot(train_losses, label="Training Loss", color='blue')
        plt.plot(val_losses, label="Validation Loss", color='orange')
        plt.xlabel("Epoch")
        plt.ylabel("Loss")
        plt.title("Training and Validation Loss Curve")
        plt.legend()
        plt.grid(True)
        plt.savefig("training_output/loss_curve_pytorch.png")
        plt.close()

        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        y_true = y_test_tensor.numpy()
        mse = mean_squared_error(y_true, y_pred)
        mae = mean_absolute_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)
        spearman_corr, _ = spearmanr(y_true, y_pred)
        pearson_corr, _ = pearsonr(y_true.flatten(), y_pred.flatten())
        residuals = y_pred.flatten() - y_true.flatten()

        # é¢„æµ‹å€¼ vs çœŸå®å€¼æŠ˜çº¿å›¾
        plt.figure()
        plt.plot(y_true, label="True Values", marker='o', color='blue')
        plt.plot(y_pred, label="Predicted Values", marker='x', color='red')
        plt.title("Predicted vs True Values (Test Set)")
        plt.xlabel("Sample Index")
        plt.ylabel("Target Value")
        plt.legend()
        plt.grid(True)
        plt.savefig("prediction_output/prediction_vs_true_test.png")
        plt.close()

        # æ®‹å·®å›¾
        plt.figure()
        plt.scatter(y_true, residuals, color='green', label="Residual Points")
        plt.axhline(0, color='red', linestyle='--', label="Zero Residual")
        plt.xlabel("True Values")
        plt.ylabel("Residuals (Predicted - True)")
        plt.title("Residual Plot (Test Set)")
        plt.grid(True)
        plt.legend()
        plt.savefig("prediction_output/residual_plot_test.png")
        plt.close()

        # æ®‹å·®ç›´æ–¹å›¾
        plt.figure()
        plt.hist(residuals, bins=10, color='orange', edgecolor='black')
        plt.title("Prediction Error Histogram (Test Set)")
        plt.xlabel("Error Value")
        plt.ylabel("Frequency")
        plt.grid(True)
        plt.savefig("prediction_output/error_histogram_test.png")
        plt.close()

        # å­˜å‚¨å„æŠ˜çš„è¯„ä¼°æŒ‡æ ‡
        all_metrics['mse'].append(mse)
        all_metrics['mae'].append(mae)
        all_metrics['r2'].append(r2)
        all_metrics['spearman'].append(spearman_corr)
        all_metrics['pearson'].append(pearson_corr)

        print(f"âœ… Fold {fold}: MSE={mse:.4f}, MAE={mae:.4f}, RÂ²={r2:.4f}, Spearman={spearman_corr:.4f},"
              f" Pearson={pearson_corr:.4f}")

    # æ±‡æ€»10æ¬¡äº¤å‰éªŒè¯çš„ç»“æœ
    print("\nğŸ“Š äº¤å‰éªŒè¯10æ¬¡ç»“æœç»Ÿè®¡ï¼š")
    with open("prediction_output/crossval_metrics_summary.txt", "w") as f:
        for metric_name in all_metrics.keys():
            mean_val = np.mean(all_metrics[metric_name])
            std_val = np.std(all_metrics[metric_name])
            result_line = f"{metric_name.upper()}: Mean={mean_val:.4f}, Std={std_val:.4f}"
            print(result_line)
            f.write(result_line + "\n")

    # ç»˜åˆ¶RÂ²åˆ†æ•°åˆ†å¸ƒç®±çº¿å›¾
    plt.figure()
    plt.boxplot(all_metrics['r2'], patch_artist=True, labels=['RÂ²'])
    plt.title('RÂ² Score Distribution Across 10 Folds')
    plt.ylabel('RÂ² Score')
    plt.grid(True)
    plt.savefig("prediction_output/r2_score_boxplot.png")
    plt.close()

    print("\nâœ… å…¨éƒ¨å®Œæˆï¼Œäº¤å‰éªŒè¯æŒ‡æ ‡å’Œå›¾è¡¨å·²ä¿å­˜ï¼")

    # ===== æ­¥éª¤7ï¼šä¿å­˜æ¨¡å‹å‚æ•° =====
    torch.save(model.state_dict(), "saved_model/battery_model.pth")  # ä¿å­˜æ¨¡å‹å‚æ•°
    print("âœ… æ¨¡å‹è®­ç»ƒä¸ä¿å­˜å®Œæˆï¼Œæ‰€æœ‰å›¾åƒä¸è¯„ä¼°æ–‡ä»¶å·²ç”Ÿæˆã€‚")
