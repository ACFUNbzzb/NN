import os
import numpy as np
import pandas as pd
import shap
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from matminer.featurizers.composition import ElementProperty
from matminer.featurizers.conversions import StrToComposition
from torch.utils.data import DataLoader, Dataset, random_split
from tqdm import tqdm
from scipy.stats import spearmanr, pearsonr

# ========= æ¨¡å—1ï¼šå‡†å¤‡åŒ–å­¦å¼æ•°æ®å¹¶æå–ç‰¹å¾ =========
def prepare_multi_system_inputs():
    # å®šä¹‰åŒ–å­¦ç³»ç»Ÿçš„åŸºæœ¬åŒ–å­¦å¼åˆ—è¡¨
    system_bases = [
        'Li26Ni27FeO54', 'Li25Ni27Fe2O54', 'Li24Ni27Fe3O54', 'Li23Ni27Fe4O54', 'Li22Ni27Fe5O53',
        'Li26Ni27O54', 'Li27Ni27O54', 'Li27Ni27O53', 'Li27Ni27O52', 'Li25Ni27CoMnO54',
        'Li26Ni27CoO54', 'Li25Ni27Co2O54', 'Li24Ni27Co3O54', 'Li23Ni27Co4O54', 'Li22Ni27Co5O53',
        'Li26Ni27MnO54', 'Li25Ni27Mn2O54', 'Li24Ni27Mn3O54', 'Li23Ni27Mn4O54', 'Li22Ni27Mn5O53',
        'Li26Ni27AlO54', 'Li25Ni27Al2O54', 'Li24Ni27Al3O54', 'Li23Ni27Al4O54', 'Li22Ni27Al5O53'
    ]
    # ä½¿ç”¨matminerä¸­çš„ElementPropertyæå–åŒ–å­¦æˆåˆ†ç‰¹å¾
    featurizer = ElementProperty.from_preset("magpie", impute_nan=True)
    X_all, y_all = [], []  # å­˜å‚¨ç‰¹å¾å’Œç›®æ ‡å€¼

    # éå†æ¯ä¸ªåŒ–å­¦å¼
    for formula in system_bases:
        try:
            # ä½¿ç”¨StrToCompositionå°†åŒ–å­¦å¼è½¬æ¢ä¸ºç»„æˆå¯¹è±¡
            df = pd.DataFrame({'formula': [formula]})
            df = StrToComposition().featurize_dataframe(df, col_id='formula', ignore_errors=True)
            # ä½¿ç”¨ElementPropertyæå–åŒ–å­¦æˆåˆ†ç‰¹å¾
            df = featurizer.featurize_dataframe(df, col_id='composition', ignore_errors=True)
            # è·å–æå–çš„ç‰¹å¾
            features = df.drop(columns=['formula', 'composition']).values[0]
            X_all.append(features)
            # äººå·¥ç”Ÿæˆç›®æ ‡å€¼ï¼ˆè¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå¯ä»¥æ ¹æ®å®é™…éœ€è¦è°ƒæ•´ï¼‰
            y_all.append(0.4 + 0.01 * len(y_all))
        except Exception:
            continue

    # å°†æ‰€æœ‰æ ·æœ¬çš„ç‰¹å¾åˆå¹¶æˆä¸€ä¸ªnumpyæ•°ç»„
    X_merged = np.array(X_all)
    y_merged = np.array(y_all)
    print("ğŸ“Š ç‰¹å¾åˆ†å¸ƒç»Ÿè®¡ä¿¡æ¯ï¼š")
    print(pd.DataFrame(X_merged).describe())  # è¾“å‡ºç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯

    return X_merged, y_merged


# ========= æ¨¡å—2ï¼šæ ‡å‡†åŒ–æ•°æ® =========
def standardize_and_align(X_raw, y_array):
    # ä½¿ç”¨StandardScalerè¿›è¡Œæ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler().fit(X_raw)
    X_scaled = scaler.transform(X_raw)
    return X_scaled, y_array, X_raw.shape[1], scaler


# ========= æ¨¡å—3ï¼šè‡ªå®šä¹‰æ•°æ®é›†ç»“æ„ =========
class MultiSystemDataset(Dataset):
    def __init__(self, X, y):
        self.X = X  # ç‰¹å¾
        self.y = y  # ç›®æ ‡å€¼

    def __len__(self):
        return len(self.y)

    def __getitem__(self, idx):
        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.float32)


# ========= æ¨¡å—4ï¼šå®šä¹‰ç¥ç»ç½‘ç»œç»“æ„ =========
class BatteryModel(nn.Module):
    def __init__(self, input_dim):
        super(BatteryModel, self).__init__()
        # å®šä¹‰ä¸€ä¸ªç®€å•çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œç»“æ„
        self.fc = nn.Sequential(
            nn.Linear(input_dim, 256),  # è¾“å…¥å±‚åˆ°éšè—å±‚
            nn.ReLU(),                  # æ¿€æ´»å‡½æ•°
            nn.Dropout(0.1),             # Dropoutæ­£åˆ™åŒ–
            nn.Linear(256, 128),         # éšè—å±‚åˆ°éšè—å±‚
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, 1)            # éšè—å±‚åˆ°è¾“å‡ºå±‚
        )

    def forward(self, x):
        return self.fc(x)


# ========= ä¸»ç¨‹åºå…¥å£ =========
if __name__ == '__main__':
    os.makedirs("training_output", exist_ok=True)  # åˆ›å»ºè®­ç»ƒè¾“å‡ºç›®å½•
    os.makedirs("prediction_output", exist_ok=True)  # åˆ›å»ºé¢„æµ‹è¾“å‡ºç›®å½•
    os.makedirs("saved_model", exist_ok=True)  # åˆ›å»ºä¿å­˜æ¨¡å‹çš„ç›®å½•
    shap.initjs()  # åˆå§‹åŒ–SHAPåº“

    # ===== æ­¥éª¤1ï¼šæ•°æ®å‡†å¤‡ =====
    X_raw, y = prepare_multi_system_inputs()  # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å€¼
    X_scaled, y, input_dim, scaler = standardize_and_align(X_raw, y)  # æ•°æ®æ ‡å‡†åŒ–
    dataset = MultiSystemDataset(X_scaled, y)  # åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_size = max(1, int(0.4 * len(dataset)))  # è®­ç»ƒé›†å¤§å°
    val_size = max(1, len(dataset) - train_size)  # éªŒè¯é›†å¤§å°
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])  # éšæœºæ‹†åˆ†æ•°æ®é›†
    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)  # è®­ç»ƒæ•°æ®åŠ è½½å™¨
    val_loader = DataLoader(val_dataset, batch_size=8)  # éªŒè¯æ•°æ®åŠ è½½å™¨

    # ===== æ­¥éª¤2ï¼šæ„å»ºæ¨¡å‹ =====
    model = BatteryModel(input_dim)  # åˆ›å»ºæ¨¡å‹
    optimizer = optim.Adam(model.parameters(), lr=0.001)  # ä½¿ç”¨Adamä¼˜åŒ–å™¨
    loss_fn = nn.MSELoss()  # å®šä¹‰æŸå¤±å‡½æ•°

    # ===== æ­¥éª¤3ï¼šè®­ç»ƒæ¨¡å‹ =====
    train_losses, val_losses = [], []  # å­˜å‚¨è®­ç»ƒæŸå¤±å’ŒéªŒè¯æŸå¤±
    for epoch in range(50):  # è®­ç»ƒ50è½®
        model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
        total_loss = 0
        # è®­ç»ƒé˜¶æ®µ
        for X_batch, y_batch in tqdm(train_loader, desc=f"Epoch {epoch + 1}/50"):
            optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦
            pred = model(X_batch)  # æ¨¡å‹é¢„æµ‹
            loss = loss_fn(pred, y_batch.view(-1, 1))  # è®¡ç®—æŸå¤±
            loss.backward()  # åå‘ä¼ æ’­
            optimizer.step()  # æ›´æ–°å‚æ•°
            total_loss += loss.item()  # ç´¯åŠ æŸå¤±
        train_losses.append(total_loss / len(train_loader))  # è®°å½•å¹³å‡è®­ç»ƒæŸå¤±

        # éªŒè¯é˜¶æ®µ
        model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        val_loss, preds, trues = 0, [], []
        with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦
            for X_batch, y_batch in val_loader:
                pred = model(X_batch)  # æ¨¡å‹é¢„æµ‹
                val_loss += loss_fn(pred, y_batch.view(-1, 1)).item()  # è®¡ç®—éªŒè¯æŸå¤±
                preds.append(pred)
                trues.append(y_batch)
        val_losses.append(val_loss / len(val_loader))  # è®°å½•å¹³å‡éªŒè¯æŸå¤±

        # è®¡ç®—RÂ²è¯„åˆ†
        r2 = r2_score(torch.cat(trues).numpy(), torch.cat(preds).numpy())
        print(f"âœ… Epoch {epoch + 1}: Train Loss={train_losses[-1]:.4f}, Val Loss={val_losses[-1]:.4f}, RÂ²={r2:.4f}")

    # ===== æ­¥éª¤4ï¼šç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿ =====
    plt.figure()
    plt.plot(train_losses, label="Training Loss", color='blue')  # è®­ç»ƒæŸå¤±æ›²çº¿
    plt.plot(val_losses, label="Validation Loss", color='orange')  # éªŒè¯æŸå¤±æ›²çº¿
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Training and Validation Loss Curve")
    plt.legend()
    plt.grid(True)
    plt.savefig("training_output/loss_curve_pytorch.png")  # ä¿å­˜æŸå¤±æ›²çº¿å›¾åƒ
    plt.close()

    # ===== æ­¥éª¤5ï¼šæ¨¡å‹è¯„ä¼°ä¸å›¾å½¢ç”Ÿæˆ =====
    X_test = torch.tensor(X_scaled[:10], dtype=torch.float32)  # æµ‹è¯•æ•°æ®
    y_test = torch.tensor(y[:10], dtype=torch.float32).view(-1, 1)  # æµ‹è¯•æ ‡ç­¾
    model.eval()
    with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦
        y_pred = model(X_test).numpy()  # é¢„æµ‹ç»“æœ

    y_true = y_test.numpy()
    residuals = y_pred.flatten() - y_true.flatten()  # è®¡ç®—æ®‹å·®

    # ğŸ“ˆ é¢„æµ‹å€¼ vs çœŸå®å€¼ï¼ˆæŠ˜çº¿å›¾ï¼‰
    plt.figure()
    plt.plot(y_true, label="True Values", marker='o', color='blue')  # çœŸå®å€¼
    plt.plot(y_pred, label="Predicted Values", marker='x', color='red')  # é¢„æµ‹å€¼
    plt.title("Predicted vs True Values (Line Plot)")
    plt.xlabel("Sample Index")
    plt.ylabel("Target Value")
    plt.legend()
    plt.grid(True)
    plt.savefig("prediction_output/prediction_vs_true.png")  # ä¿å­˜é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„æŠ˜çº¿å›¾
    plt.close()

    # ğŸ“‰ æ®‹å·®å›¾
    plt.figure()
    plt.scatter(y_true, residuals, color='green', label="Residual Points")  # æ®‹å·®ç‚¹
    plt.axhline(0, color='red', linestyle='--', label="Zero Residual")  # 0æ®‹å·®çº¿
    plt.xlabel("True Values")
    plt.ylabel("Residuals (Predicted - True)")
    plt.title("Residual Plot")
    plt.grid(True)
    plt.legend()
    plt.savefig("prediction_output/residual_plot.png")  # ä¿å­˜æ®‹å·®å›¾
    plt.close()

    # ğŸ“Š æ®‹å·®ç›´æ–¹å›¾
    plt.figure()
    plt.hist(residuals, bins=10, color='orange', edgecolor='black')  # æ®‹å·®ç›´æ–¹å›¾
    plt.title("Prediction Error Histogram")
    plt.xlabel("Error Value")
    plt.ylabel("Frequency")
    plt.grid(True)
    plt.savefig("prediction_output/error_histogram.png")  # ä¿å­˜æ®‹å·®ç›´æ–¹å›¾
    plt.close()

    # ğŸ“„ ä¿å­˜è¯„ä¼°æŒ‡æ ‡
    mse = mean_squared_error(y_true, y_pred)  # è®¡ç®—å‡æ–¹è¯¯å·®
    mae = mean_absolute_error(y_true, y_pred)  # è®¡ç®—å¹³å‡ç»å¯¹è¯¯å·®
    r2 = r2_score(y_true, y_pred)  # è®¡ç®—RÂ²å¾—åˆ†
    spearman_corr, _ = spearmanr(y_true, y_pred)  # è®¡ç®—Spearmanç›¸å…³ç³»æ•°
    pearson_corr, _ = pearsonr(y_true.flatten(), y_pred.flatten())  # è®¡ç®—Pearsonç›¸å…³ç³»æ•°
    with open("prediction_output/evaluation_metrics.txt", "w") as f:
        f.write(f"MSE: {mse:.4f}\nMAE: {mae:.4f}\nRÂ²: {r2:.4f}\n")
        f.write(f"Spearman: {spearman_corr:.4f}\nPearson: {pearson_corr:.4f}\n")  # ä¿å­˜è¯„ä¼°æŒ‡æ ‡

    # ===== æ­¥éª¤7ï¼šä¿å­˜æ¨¡å‹å‚æ•° =====
    torch.save(model.state_dict(), "saved_model/battery_model.pth")  # ä¿å­˜æ¨¡å‹å‚æ•°
    print("âœ… æ¨¡å‹è®­ç»ƒä¸ä¿å­˜å®Œæˆï¼Œæ‰€æœ‰å›¾åƒä¸è¯„ä¼°æ–‡ä»¶å·²ç”Ÿæˆã€‚")
